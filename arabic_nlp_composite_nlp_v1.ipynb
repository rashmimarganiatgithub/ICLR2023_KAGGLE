{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-06-02T13:52:03.896453Z",
     "iopub.status.busy": "2021-06-02T13:52:03.896085Z",
     "iopub.status.idle": "2021-06-02T13:52:47.3357Z",
     "shell.execute_reply": "2021-06-02T13:52:47.334231Z",
     "shell.execute_reply.started": "2021-06-02T13:52:03.896422Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "from torch.nn import TransformerDecoder, TransformerDecoderLayer\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import math\n",
    "import random\n",
    "\n",
    "import os\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "from transformers import AutoModel\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler, Dataset, Sampler\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code imports various Python packages and libraries and assigns them aliases to be used later in the code:\n",
    "\n",
    "    import pandas as pd: imports the Pandas library with the alias pd.\n",
    "    from torch.nn import TransformerEncoder, TransformerEncoderLayer: imports the TransformerEncoder and TransformerEncoderLayer classes from the PyTorch nn module.\n",
    "    from torch.nn import TransformerDecoder, TransformerDecoderLayer: imports the TransformerDecoder and TransformerDecoderLayer classes from the PyTorch nn module.\n",
    "    import torch.nn.functional as F: imports the functional API of the PyTorch nn module with the alias F.\n",
    "    import torch imports the PyTorch library.\n",
    "    import torch.nn as nn: imports the neural network module of PyTorch with the alias nn.\n",
    "    import torch.optim as optim: imports the PyTorch optimizer module with the alias optim.\n",
    "    import numpy as np: imports the NumPy library with the alias np.\n",
    "    import math: imports the math module of Python.\n",
    "    import random: imports the random module of Python.\n",
    "    import os: imports the os module of Python.\n",
    "    import re: imports the re module of Python.\n",
    "    from tqdm import tqdm: imports the tqdm module to display progress bars.\n",
    "\n",
    "Then the code imports several classes and functions from the transformers module:\n",
    "\n",
    "    from transformers import AutoModel: imports the AutoModel class.\n",
    "    from transformers import AutoTokenizer: imports the AutoTokenizer class.\n",
    "    from transformers import AdamW, get_linear_schedule_with_warmup: imports the AdamW optimizer and the get_linear_schedule_with_warmup function.\n",
    "\n",
    "After that, the code imports more modules and libraries:\n",
    "\n",
    "    from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler, Dataset, Sampler: imports several classes and functions from the PyTorch utils.data module.\n",
    "    import numpy as np: imports the NumPy library again with the same alias np.\n",
    "    import pandas as pd: imports the Pandas library again with the same alias pd.\n",
    "    import matplotlib.pyplot as plt: imports the pyplot module from the Matplotlib library with the alias plt.\n",
    "    from sklearn.model_selection import KFold: imports the KFold class from the model_selection module of the scikit-learn library.\n",
    "\n",
    "Finally, the code imports the time module of Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-02T13:52:47.338591Z",
     "iopub.status.busy": "2021-06-02T13:52:47.338117Z",
     "iopub.status.idle": "2021-06-02T13:52:47.344119Z",
     "shell.execute_reply": "2021-06-02T13:52:47.342676Z",
     "shell.execute_reply.started": "2021-06-02T13:52:47.338542Z"
    }
   },
   "outputs": [],
   "source": [
    "#The important parameters\n",
    "model_name_ar = 'moha/arabert_c19'\n",
    "model_name_en = 'bert-base-uncased'\n",
    "\n",
    "batch_size = 32\n",
    "n_epochs = 3\n",
    "\n",
    "seed = 99 #Important for reproducing the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**model_name_ar** and **model_name_en** specify the names of the pre-trained models that will be used for training the language model. The arabert_c19 model is a pre-trained BERT model for Arabic, while bert-base-uncased is a pre-trained BERT model for English.\n",
    "\n",
    "**batch_size** determines the number of samples that will be fed to the model at once during training. A larger batch size can speed up training, but can also require more memory.\n",
    "\n",
    "**n_epochs** specifies the number of times the model will go through the entire training dataset during training.\n",
    "\n",
    "**seed** is important for ensuring reproducibility of the results. Setting a seed means that the random number generator used during training will produce the same sequence of random numbers each time the model is trained, which can help ensure that the results are consistent across different runs\n",
    "\n",
    "**NOTE:** For Swahili, there also exists several pre-trained models available for the Swahili language, which is also known as Kiswahili. Here are a few options:\n",
    "\n",
    " **bert-base-swahili-cased:** This is a pre-trained BERT model for Swahili, which is available on the Hugging Face model hub.\n",
    "**ai4d-afnlp-swahili-tpu:** his is a pre-trained RoBERTa model for Swahili, which was trained by the African Language Technology (ALT) research group and is also available on the Hugging Face model hub.\n",
    "**swahili-gpt2:** This is a pre-trained GPT-2 model for Swahili, which was trained by the Masakhane community."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-02T13:52:47.345896Z",
     "iopub.status.busy": "2021-06-02T13:52:47.345589Z",
     "iopub.status.idle": "2021-06-02T13:53:01.181887Z",
     "shell.execute_reply": "2021-06-02T13:53:01.180701Z",
     "shell.execute_reply.started": "2021-06-02T13:52:47.345866Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer_ar = AutoTokenizer.from_pretrained(model_name_ar, do_lower_case=True)\n",
    "tokenizer_en = AutoTokenizer.from_pretrained(model_name_en, do_lower_case=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These lines of code are used to create tokenizers for the Arabic and English languages, respectively, using the pre-trained models specified by model_name_ar and model_name_en.\n",
    "\n",
    "The AutoTokenizer class is part of the Hugging Face Transformers library, which provides a simple way to download and use pre-trained language models. By calling the from_pretrained() method and passing in the name of the pre-trained model, the tokenizer will be automatically downloaded and initialized.\n",
    "\n",
    "The do_lower_case=True argument specifies that the text should be lowercased before tokenization. This is often done in NLP tasks to reduce the vocabulary size and improve generalization, although it is worth noting that case can carry important information in some languages.\n",
    "\n",
    "After creating the tokenizers, they can be used to convert raw text into numerical tokens that can be fed into a neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** Using a pre-trained model during tokenization can have some negative consequences depending on the specific use case. Here are a few potential issues to consider:\n",
    "\n",
    "    Lack of domain-specific vocabulary: Pre-trained models are typically trained on a large, general-purpose corpus of text, which may not include domain-specific vocabulary that is important for a particular task. This can lead to the model tokenizing important domain-specific terms as out-of-vocabulary tokens or incorrectly assigning them to a more general token.\n",
    "\n",
    "    Encoding errors: Pre-trained models may not be able to correctly encode certain types of characters or scripts. For example, if the pre-trained model was trained on a corpus that doesn't include certain diacritic marks, it may not be able to accurately tokenize or encode text that contains those marks.\n",
    "\n",
    "    Privacy concerns: Pre-trained models are often trained on large amounts of text data, which may include sensitive information. When using a pre-trained tokenizer, there is a risk that sensitive information could be inadvertently included in the tokenized output, which could have privacy implications.\n",
    "\n",
    "    Unintended biases: Pre-trained models can inherit biases from the data they were trained on, and these biases can manifest themselves in the tokenization process. For example, if the pre-trained model was trained on text that includes a biased representation of a certain group of people, the model may tokenize words associated with that group in a biased way.\n",
    "\n",
    "It's important to carefully evaluate the potential drawbacks of using a pre-trained model during tokenization and consider alternative approaches if necessary. In some cases, it may be necessary to fine-tune the tokenizer on a domain-specific corpus or manually curate the vocabulary to avoid these issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-02T13:53:01.192636Z",
     "iopub.status.busy": "2021-06-02T13:53:01.192236Z",
     "iopub.status.idle": "2021-06-02T13:53:01.207015Z",
     "shell.execute_reply": "2021-06-02T13:53:01.206017Z",
     "shell.execute_reply.started": "2021-06-02T13:53:01.192598Z"
    }
   },
   "outputs": [],
   "source": [
    "def set_seed():\n",
    "    \"\"\"Set seed for reproducibility.\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    \n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    \n",
    "set_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-02T13:53:01.209287Z",
     "iopub.status.busy": "2021-06-02T13:53:01.208855Z",
     "iopub.status.idle": "2021-06-02T13:53:03.987754Z",
     "shell.execute_reply": "2021-06-02T13:53:03.986452Z",
     "shell.execute_reply.started": "2021-06-02T13:53:01.209241Z"
    }
   },
   "outputs": [],
   "source": [
    "xls = pd.ExcelFile(\"../data/external/transliteration/dataset.xlsx\")\n",
    "dataset = pd.read_excel(xls, \"Sheet1\")\n",
    "\n",
    "known = dataset[dataset.from_source == True]\n",
    "dataset = dataset[[\"arabizi\", \"arabic\", \"from_source\"]]\n",
    "\n",
    "dataset.columns = [\"Arabize\", \"Arabic\", \"from_source\"]\n",
    "\n",
    "#dataset.  #Drop Arabic duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-02T13:53:03.989821Z",
     "iopub.status.busy": "2021-06-02T13:53:03.989368Z",
     "iopub.status.idle": "2021-06-02T13:53:04.003371Z",
     "shell.execute_reply": "2021-06-02T13:53:04.001919Z",
     "shell.execute_reply.started": "2021-06-02T13:53:03.989771Z"
    }
   },
   "outputs": [],
   "source": [
    "#Store known words so we just replace them instead of computing them with model again\n",
    "#This saves up computation time, and improves transliteration accuracy\n",
    "known = known[[\"arabizi\", \"arabic\"]].set_index(\"arabizi\", drop=True).arabic.to_dict()\n",
    "known_idx = list(known.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-02T13:53:04.005475Z",
     "iopub.status.busy": "2021-06-02T13:53:04.005102Z",
     "iopub.status.idle": "2021-06-02T13:53:04.594768Z",
     "shell.execute_reply": "2021-06-02T13:53:04.593354Z",
     "shell.execute_reply.started": "2021-06-02T13:53:04.005438Z"
    }
   },
   "outputs": [],
   "source": [
    "in_max = dataset.apply(lambda x: len(str(x.Arabize)), axis=1).max()\n",
    "out_max = dataset.apply(lambda x: len(x.Arabic), axis=1).max() + 2  #Take into account eos and sos\n",
    "\n",
    "pad_token = 0\n",
    "eos_token = 2\n",
    "sos_token = 1\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-02T13:53:04.596889Z",
     "iopub.status.busy": "2021-06-02T13:53:04.596542Z",
     "iopub.status.idle": "2021-06-02T13:53:04.605661Z",
     "shell.execute_reply": "2021-06-02T13:53:04.604277Z",
     "shell.execute_reply.started": "2021-06-02T13:53:04.596855Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess(a):\n",
    "        \n",
    "    x = a.copy()  \n",
    "    \n",
    "    def filter_letters_arabizi(word):\n",
    "        \n",
    "        word = word.replace(\"$\", \"s\")\n",
    "        word = word.replace(\"å\", \"a\")\n",
    "        word = word.replace(\"é\", \"e\")\n",
    "        word = word.replace(\"ê\", \"e\")\n",
    "        word = word.replace(\"ÿ\", \"y\")\n",
    "        word = word.replace(\"ą\", \"a\")\n",
    "        word = word.replace(\"ī\", \"i\")\n",
    "        word = word.replace(\"\\n\", \"\")\n",
    "        word = word.replace(\"′\", \"'\")\n",
    "        \n",
    "        return word\n",
    "    \n",
    "    x.Arabize = filter_letters_arabizi(str(x.Arabize))\n",
    "    x.Arabic = x.Arabic\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-02T13:53:04.608063Z",
     "iopub.status.busy": "2021-06-02T13:53:04.607601Z",
     "iopub.status.idle": "2021-06-02T13:53:08.040548Z",
     "shell.execute_reply": "2021-06-02T13:53:08.039363Z",
     "shell.execute_reply.started": "2021-06-02T13:53:04.608016Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset[[\"Arabize\",\"Arabic\"]] = dataset[[\"Arabize\",\"Arabic\"]].apply(preprocess, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-02T13:53:08.041925Z",
     "iopub.status.busy": "2021-06-02T13:53:08.041652Z",
     "iopub.status.idle": "2021-06-02T13:53:08.059417Z",
     "shell.execute_reply": "2021-06-02T13:53:08.058574Z",
     "shell.execute_reply.started": "2021-06-02T13:53:08.041898Z"
    }
   },
   "outputs": [],
   "source": [
    "in_tokens = set(\" \".join(dataset.Arabize.values.tolist()).lower())\n",
    "in_token_to_int = {token: (i+1) for i,token in enumerate(sorted(in_tokens))}\n",
    "\n",
    "in_token_to_int[0] = \"<pad>\"\n",
    "\n",
    "out_tokens = set(\" \".join(dataset.Arabic.values.tolist()))\n",
    "out_token_to_int = {token: (i+3) for i,token in enumerate(sorted(out_tokens))}\n",
    "\n",
    "\n",
    "\n",
    "out_token_to_int[\"<pad>\"] = pad_token\n",
    "\n",
    "out_token_to_int[\"<sos>\"] = sos_token\n",
    "out_token_to_int[\"<eos>\"] = eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-02T13:53:08.060843Z",
     "iopub.status.busy": "2021-06-02T13:53:08.060405Z",
     "iopub.status.idle": "2021-06-02T13:53:08.147449Z",
     "shell.execute_reply": "2021-06-02T13:53:08.145903Z",
     "shell.execute_reply.started": "2021-06-02T13:53:08.060765Z"
    }
   },
   "outputs": [],
   "source": [
    "def tokenize(a):\n",
    "    \n",
    "    x = a.copy()\n",
    "    \n",
    "    x.Arabize = [in_token_to_int[i] for i in x.Arabize.lower()]\n",
    "    x.Arabic = [sos_token] + [out_token_to_int[i] for i in x.Arabic] + [eos_token]\n",
    "    \n",
    "    x.Arabize = x.Arabize + (in_max - len(x.Arabize)) * [pad_token] \n",
    "    x.Arabic = x.Arabic + (out_max - len(x.Arabic)) * [pad_token] \n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-02T13:53:08.149015Z",
     "iopub.status.busy": "2021-06-02T13:53:08.148689Z",
     "iopub.status.idle": "2021-06-02T13:53:12.724394Z",
     "shell.execute_reply": "2021-06-02T13:53:12.723416Z",
     "shell.execute_reply.started": "2021-06-02T13:53:08.148985Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset[[\"Arabize\",\"Arabic\"]] = dataset[[\"Arabize\",\"Arabic\"]].apply(tokenize, axis=1)\n",
    "\n",
    "validation = dataset.sample(frac=0.1)\n",
    "train = dataset.drop(validation.index)\n",
    "\n",
    "X_train = train.Arabize\n",
    "y_train = train.Arabic\n",
    "\n",
    "X_valid = validation.Arabize\n",
    "y_valid = validation.Arabic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-02T13:53:12.726342Z",
     "iopub.status.busy": "2021-06-02T13:53:12.725912Z",
     "iopub.status.idle": "2021-06-02T13:53:12.739607Z",
     "shell.execute_reply": "2021-06-02T13:53:12.738278Z",
     "shell.execute_reply.started": "2021-06-02T13:53:12.726298Z"
    }
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=9000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.scale = nn.Parameter(torch.ones(1))\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(\n",
    "            0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.scale * self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-02T13:53:12.741728Z",
     "iopub.status.busy": "2021-06-02T13:53:12.741388Z",
     "iopub.status.idle": "2021-06-02T13:53:12.759349Z",
     "shell.execute_reply": "2021-06-02T13:53:12.758216Z",
     "shell.execute_reply.started": "2021-06-02T13:53:12.741697Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, intoken, outtoken ,hidden, enc_layers=1, dec_layers=1, dropout=0.15, nheads=4):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        \n",
    "        ff_model = hidden*4\n",
    "        \n",
    "        self.encoder = nn.Embedding(intoken, hidden)\n",
    "        self.pos_encoder = PositionalEncoding(hidden, dropout)\n",
    "\n",
    "        self.decoder = nn.Embedding(outtoken, hidden) \n",
    "        self.pos_decoder = PositionalEncoding(hidden, dropout)\n",
    "        \n",
    "        \n",
    "        encoder_layers = TransformerEncoderLayer(d_model=hidden, nhead = nheads, dim_feedforward = ff_model, dropout=dropout, activation='relu')\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, enc_layers)\n",
    "\n",
    "        encoder_layers = TransformerDecoderLayer(hidden, nheads, ff_model, dropout, activation='relu')\n",
    "        self.transformer_decoder = TransformerDecoder(encoder_layers, dec_layers)        \n",
    "\n",
    "        self.fc_out = nn.Linear(hidden, outtoken)\n",
    "\n",
    "        self.src_mask = None\n",
    "        self.trg_mask = None\n",
    "        self.memory_mask = None\n",
    "\n",
    "        \n",
    "    def generate_square_subsequent_mask(self, sz, sz1=None):\n",
    "        \n",
    "        if sz1 == None:\n",
    "            mask = torch.triu(torch.ones(sz, sz), 1)\n",
    "        else:\n",
    "            mask = torch.triu(torch.ones(sz, sz1), 1)\n",
    "            \n",
    "        return mask.masked_fill(mask==1, float('-inf'))\n",
    "\n",
    "    def make_len_mask_enc(self, inp):\n",
    "        return (inp == pad_token).transpose(0, 1)   #(batch_size, output_seq_len)\n",
    "    \n",
    "    def make_len_mask_dec(self, inp):\n",
    "        return (inp == pad_token).transpose(0, 1) #(batch_size, input_seq_len)\n",
    "    \n",
    "\n",
    "\n",
    "    def forward(self, src, trg): #SRC: (seq_len, batch_size)\n",
    "\n",
    "        if self.trg_mask is None or self.trg_mask.size(0) != len(trg):\n",
    "            self.trg_mask = self.generate_square_subsequent_mask(len(trg)).to(trg.device)\n",
    "            \n",
    "\n",
    "        #Adding padding mask\n",
    "        src_pad_mask = self.make_len_mask_enc(src)\n",
    "        trg_pad_mask = self.make_len_mask_dec(trg)\n",
    "             \n",
    "\n",
    "        #Add embeddings Encoder\n",
    "        src = self.encoder(src)  #Embedding, (seq_len, batch_size, d_model)\n",
    "        src = self.pos_encoder(src)   #Pos embedding\n",
    "        \n",
    "        \n",
    "        #Add embedding decoder\n",
    "        trg = self.decoder(trg) #(seq_len, batch_size, d_model)\n",
    "        trg = self.pos_decoder(trg)\n",
    "\n",
    "        \n",
    "        memory = self.transformer_encoder(src, None, src_pad_mask)\n",
    "        output = self.transformer_decoder(tgt = trg, memory = memory, tgt_mask = self.trg_mask, memory_mask = None, \n",
    "                                          tgt_key_padding_mask = trg_pad_mask, memory_key_padding_mask = src_pad_mask)\n",
    "\n",
    "        output = self.fc_out(output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-02T13:53:12.761117Z",
     "iopub.status.busy": "2021-06-02T13:53:12.760806Z",
     "iopub.status.idle": "2021-06-02T13:53:12.777387Z",
     "shell.execute_reply": "2021-06-02T13:53:12.776267Z",
     "shell.execute_reply.started": "2021-06-02T13:53:12.761085Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(in_token_to_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-02T13:53:12.779577Z",
     "iopub.status.busy": "2021-06-02T13:53:12.779153Z",
     "iopub.status.idle": "2021-06-02T13:53:12.790113Z",
     "shell.execute_reply": "2021-06-02T13:53:12.789263Z",
     "shell.execute_reply.started": "2021-06-02T13:53:12.779537Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(out_token_to_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed()\n",
    "model = TransformerModel(len(in_token_to_int), len(out_token_to_int), 128).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoamOpt:\n",
    "    \"Optim wrapper that implements rate.\"\n",
    "    def __init__(self, model_size, factor, warmup, optimizer):\n",
    "        self.optimizer = optimizer\n",
    "        self._step = 0\n",
    "        self.warmup = warmup\n",
    "        self.factor = factor\n",
    "        self.model_size = model_size\n",
    "        self._rate = 0\n",
    "        \n",
    "    def step(self):\n",
    "        \"Update parameters and rate\"\n",
    "        self._step += 1\n",
    "        rate = self.rate()\n",
    "        for p in self.optimizer.param_groups:\n",
    "            p['lr'] = rate\n",
    "        self._rate = rate\n",
    "        self.optimizer.step()\n",
    "        \n",
    "    def rate(self, step = None):\n",
    "        \"Implement `lrate` above\"\n",
    "        if step is None:\n",
    "            step = self._step\n",
    "        return self.factor * \\\n",
    "            (self.model_size ** (-0.5) *\n",
    "            min(step ** (-0.5), step * self.warmup ** (-1.5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Arab2ArabizDS(Dataset):\n",
    "\n",
    "    def __init__(self, data, label):\n",
    "        \n",
    "        self.data = data.values.tolist()\n",
    "        self.labels = label.values.tolist()\n",
    "        \n",
    "        self.lengths_source = [len(i) for i in data]\n",
    "        self.lengths_label = [len(i) for i in label]\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return (self.data[idx], self.labels[idx], self.lengths_source[idx], self.lengths_label[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_collator_Arab2Arabiz(data):\n",
    "    \n",
    "    word, label, length_source, length_label = zip(*data)\n",
    "    \n",
    "    tensor_dim_1 = max(length_source)\n",
    "    tensor_dim_2 = max(length_label)\n",
    "    \n",
    "    out_word = torch.full((len(word), tensor_dim_1), dtype=torch.long, fill_value=pad_token)\n",
    "    label_word = torch.full((len(word), tensor_dim_2), dtype=torch.long, fill_value=pad_token)\n",
    "\n",
    "    for i in range(len(word)):\n",
    "        \n",
    "        out_word[i][:len(word[i])] = torch.Tensor(word[i])\n",
    "        label_word[i][:len(label[i])] = torch.Tensor(label[i])\n",
    "    \n",
    "    return (out_word, label_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KSampler(Sampler):\n",
    "\n",
    "    def __init__(self, data_source, batch_size):\n",
    "        self.lens = [x[1] for x in data_source]\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __iter__(self):\n",
    "\n",
    "        idx = list(range(len(self.lens)))\n",
    "        arr = list(zip(self.lens, idx))\n",
    "\n",
    "        random.shuffle(arr)\n",
    "        n = self.batch_size*100\n",
    "\n",
    "        iterator = []\n",
    "\n",
    "        for i in range(0, len(self.lens), n):\n",
    "            dt = arr[i:i+n]\n",
    "            dt = sorted(dt, key=lambda x: x[0])\n",
    "\n",
    "            for j in range(0, len(dt), self.batch_size):\n",
    "                indices = list(map(lambda x: x[1], dt[j:j+self.batch_size]))\n",
    "                iterator.append(indices)\n",
    "\n",
    "        random.shuffle(iterator)\n",
    "        return iter([item for sublist in iterator for item in sublist])  #Flatten nested list\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.lens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_worker(worker_id):\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    numpy.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_data = Arab2ArabizDS(X_train, y_train)\n",
    "train_sampler = KSampler(train_data, batch_size)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size, worker_init_fn=seed_worker, collate_fn=data_collator_Arab2Arabiz)\n",
    "\n",
    "valid_data = Arab2ArabizDS(X_valid, y_valid)\n",
    "valid_sampler = KSampler(valid_data, batch_size)\n",
    "valid_dataloader = DataLoader(valid_data, sampler=valid_sampler, batch_size=batch_size,worker_init_fn=seed_worker, collate_fn=data_collator_Arab2Arabiz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_token)\n",
    "optimizer = NoamOpt(128, 1, 4000 ,optim.Adam(model.parameters(), lr=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(iterator):\n",
    "    \n",
    "    total_loss = 0\n",
    "\n",
    "    for src, trg in iterator:\n",
    "\n",
    "        src = src.T.to(device)\n",
    "        trg = trg.T.to(device)\n",
    "\n",
    "        output = model(src, trg[:-1, :])\n",
    "        output = output.reshape(-1, output.shape[2])\n",
    "\n",
    "        optimizer.optimizer.zero_grad()\n",
    "        loss = criterion(output, trg[1:].reshape(-1))\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "    return total_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_validation(iterator):\n",
    "    \n",
    "    total_loss = 0\n",
    "\n",
    "    for src, trg in iterator:\n",
    "\n",
    "        src = src.T.to(device)\n",
    "        trg = trg.T.to(device)\n",
    "\n",
    "        output = model(src, trg[:-1, :])\n",
    "        output = output.reshape(-1, output.shape[2])\n",
    "\n",
    "        optimizer.optimizer.zero_grad()\n",
    "        loss = criterion(output, trg[1:].reshape(-1))\n",
    "        total_loss += loss.item()\n",
    "\n",
    "\n",
    "    return total_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 0 -- 3.123360 -- Val Loss: 2.228412\n",
      "EPOCH 1 -- 1.511698 -- Val Loss: 1.136146\n",
      "EPOCH 2 -- 1.014048 -- Val Loss: 0.923969\n",
      "EPOCH 3 -- 0.885723 -- Val Loss: 0.829728\n",
      "EPOCH 4 -- 0.823842 -- Val Loss: 0.863400\n",
      "EPOCH 5 -- 0.789844 -- Val Loss: 0.780268\n",
      "EPOCH 6 -- 0.755596 -- Val Loss: 0.781666\n",
      "EPOCH 7 -- 0.737392 -- Val Loss: 0.778635\n",
      "EPOCH 8 -- 0.724340 -- Val Loss: 0.777418\n",
      "EPOCH 9 -- 0.691906 -- Val Loss: 0.682172\n",
      "EPOCH 10 -- 0.659702 -- Val Loss: 0.701159\n",
      "EPOCH 11 -- 0.634357 -- Val Loss: 0.634416\n",
      "EPOCH 12 -- 0.606610 -- Val Loss: 0.620484\n",
      "EPOCH 13 -- 0.589926 -- Val Loss: 0.610749\n",
      "EPOCH 14 -- 0.578258 -- Val Loss: 0.596723\n",
      "EPOCH 15 -- 0.559159 -- Val Loss: 0.588234\n",
      "EPOCH 16 -- 0.542263 -- Val Loss: 0.575900\n",
      "EPOCH 17 -- 0.536076 -- Val Loss: 0.563821\n",
      "EPOCH 18 -- 0.527246 -- Val Loss: 0.566686\n",
      "EPOCH 19 -- 0.510614 -- Val Loss: 0.571389\n",
      "EPOCH 20 -- 0.504423 -- Val Loss: 0.533411\n",
      "EPOCH 21 -- 0.494509 -- Val Loss: 0.555045\n",
      "EPOCH 22 -- 0.486215 -- Val Loss: 0.539129\n",
      "EPOCH 23 -- 0.479624 -- Val Loss: 0.517323\n",
      "EPOCH 24 -- 0.474335 -- Val Loss: 0.536851\n",
      "EPOCH 25 -- 0.471370 -- Val Loss: 0.517326\n",
      "EPOCH 26 -- 0.461384 -- Val Loss: 0.520885\n",
      "EPOCH 27 -- 0.458250 -- Val Loss: 0.501752\n",
      "EPOCH 28 -- 0.453710 -- Val Loss: 0.514616\n",
      "EPOCH 29 -- 0.446159 -- Val Loss: 0.504582\n",
      "EPOCH 30 -- 0.440099 -- Val Loss: 0.496673\n",
      "EPOCH 31 -- 0.435745 -- Val Loss: 0.507421\n",
      "EPOCH 32 -- 0.432361 -- Val Loss: 0.493647\n",
      "EPOCH 33 -- 0.430677 -- Val Loss: 0.493088\n",
      "EPOCH 34 -- 0.421207 -- Val Loss: 0.484305\n",
      "EPOCH 35 -- 0.417375 -- Val Loss: 0.492162\n",
      "EPOCH 36 -- 0.415935 -- Val Loss: 0.481921\n",
      "EPOCH 37 -- 0.410692 -- Val Loss: 0.476104\n",
      "EPOCH 38 -- 0.408857 -- Val Loss: 0.489242\n",
      "EPOCH 39 -- 0.404621 -- Val Loss: 0.483651\n",
      "EPOCH 40 -- 0.402956 -- Val Loss: 0.485492\n",
      "EPOCH 41 -- 0.397503 -- Val Loss: 0.482860\n",
      "EPOCH 42 -- 0.396618 -- Val Loss: 0.474574\n",
      "EPOCH 43 -- 0.397408 -- Val Loss: 0.469581\n",
      "EPOCH 44 -- 0.389968 -- Val Loss: 0.462251\n",
      "EPOCH 45 -- 0.387099 -- Val Loss: 0.476675\n",
      "EPOCH 46 -- 0.385429 -- Val Loss: 0.474230\n",
      "EPOCH 47 -- 0.385160 -- Val Loss: 0.463720\n",
      "EPOCH 48 -- 0.379222 -- Val Loss: 0.463794\n",
      "EPOCH 49 -- 0.378010 -- Val Loss: 0.471673\n",
      "EPOCH 50 -- 0.372500 -- Val Loss: 0.461171\n",
      "EPOCH 51 -- 0.368906 -- Val Loss: 0.467255\n",
      "EPOCH 52 -- 0.372291 -- Val Loss: 0.476328\n",
      "EPOCH 53 -- 0.366189 -- Val Loss: 0.467708\n",
      "EPOCH 54 -- 0.363273 -- Val Loss: 0.462721\n",
      "EPOCH 55 -- 0.357796 -- Val Loss: 0.452433\n",
      "EPOCH 56 -- 0.361114 -- Val Loss: 0.457579\n",
      "EPOCH 57 -- 0.357708 -- Val Loss: 0.460023\n",
      "EPOCH 58 -- 0.359783 -- Val Loss: 0.446974\n",
      "EPOCH 59 -- 0.354110 -- Val Loss: 0.452504\n",
      "EPOCH 60 -- 0.354263 -- Val Loss: 0.455200\n",
      "EPOCH 61 -- 0.352000 -- Val Loss: 0.462739\n",
      "EPOCH 62 -- 0.350811 -- Val Loss: 0.452710\n",
      "EPOCH 63 -- 0.350396 -- Val Loss: 0.443670\n",
      "EPOCH 64 -- 0.340637 -- Val Loss: 0.447481\n",
      "EPOCH 65 -- 0.345151 -- Val Loss: 0.444104\n",
      "EPOCH 66 -- 0.341669 -- Val Loss: 0.435134\n",
      "EPOCH 67 -- 0.341126 -- Val Loss: 0.435169\n",
      "EPOCH 68 -- 0.337976 -- Val Loss: 0.453772\n",
      "EPOCH 69 -- 0.336749 -- Val Loss: 0.445205\n",
      "EPOCH 70 -- 0.334024 -- Val Loss: 0.449421\n",
      "EPOCH 71 -- 0.333626 -- Val Loss: 0.439268\n",
      "EPOCH 72 -- 0.330031 -- Val Loss: 0.446629\n",
      "EPOCH 73 -- 0.329435 -- Val Loss: 0.448067\n",
      "EPOCH 74 -- 0.330538 -- Val Loss: 0.444798\n",
      "EPOCH 75 -- 0.330991 -- Val Loss: 0.434328\n",
      "EPOCH 76 -- 0.328004 -- Val Loss: 0.443016\n",
      "EPOCH 77 -- 0.329658 -- Val Loss: 0.446379\n",
      "EPOCH 78 -- 0.323104 -- Val Loss: 0.446473\n",
      "EPOCH 79 -- 0.326192 -- Val Loss: 0.443791\n",
      "EPOCH 80 -- 0.324286 -- Val Loss: 0.451904\n",
      "EPOCH 81 -- 0.320857 -- Val Loss: 0.438758\n",
      "EPOCH 82 -- 0.319329 -- Val Loss: 0.443614\n",
      "EPOCH 83 -- 0.319112 -- Val Loss: 0.446730\n",
      "EPOCH 84 -- 0.317843 -- Val Loss: 0.438387\n",
      "EPOCH 85 -- 0.317715 -- Val Loss: 0.431415\n",
      "EPOCH 86 -- 0.317139 -- Val Loss: 0.444147\n",
      "EPOCH 87 -- 0.312047 -- Val Loss: 0.452574\n",
      "EPOCH 88 -- 0.312573 -- Val Loss: 0.445827\n",
      "EPOCH 89 -- 0.311626 -- Val Loss: 0.446205\n",
      "EPOCH 90 -- 0.311181 -- Val Loss: 0.428946\n",
      "EPOCH 91 -- 0.310631 -- Val Loss: 0.435142\n",
      "EPOCH 92 -- 0.310622 -- Val Loss: 0.430065\n",
      "EPOCH 93 -- 0.305902 -- Val Loss: 0.436287\n",
      "EPOCH 94 -- 0.305548 -- Val Loss: 0.428907\n",
      "EPOCH 95 -- 0.308321 -- Val Loss: 0.436570\n",
      "EPOCH 96 -- 0.302437 -- Val Loss: 0.436143\n",
      "EPOCH 97 -- 0.304774 -- Val Loss: 0.435723\n",
      "EPOCH 98 -- 0.303482 -- Val Loss: 0.439224\n",
      "EPOCH 99 -- 0.302232 -- Val Loss: 0.418394\n"
     ]
    }
   ],
   "source": [
    "set_seed()\n",
    "\n",
    "\n",
    "min_loss = 99\n",
    "#Change model size \n",
    "for i in range(100):\n",
    "    \n",
    "    loss = run_epoch(train_dataloader)\n",
    "    loss_val = run_validation(valid_dataloader)\n",
    "    \n",
    "    if loss_val < min_loss:\n",
    "        min_loss = loss_val\n",
    "        torch.save(model, \"convert_best\")\n",
    "    \n",
    "    print(\"EPOCH %d -- %f -- Val Loss: %f\" % (i, loss, loss_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(\"convert_best\").eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4183937519226434"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_int_to_token = {out_token_to_int[t]:t for t in out_token_to_int}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arabizi_2_arabic(inp):\n",
    "    \n",
    "    input_sentence = [in_token_to_int[i] for i in inp.lower()]\n",
    "    preds = [sos_token]\n",
    "\n",
    "    input_sentence = torch.Tensor(input_sentence).unsqueeze(-1).long().to(device)\n",
    "\n",
    "\n",
    "    new_char = -1\n",
    "\n",
    "    while new_char != eos_token:\n",
    "\n",
    "        output_sentence = torch.Tensor(preds).unsqueeze(-1).long().to(device)\n",
    "\n",
    "        src = model.pos_encoder(model.encoder(input_sentence))\n",
    "        trg = model.pos_encoder(model.decoder(output_sentence))\n",
    "\n",
    "        memory = model.transformer_encoder(src)\n",
    "        output = model.transformer_decoder(tgt = trg, memory = memory)\n",
    "\n",
    "        output = model.fc_out(output)\n",
    "        new_char = output.argmax(-1)[-1, 0].item()\n",
    "\n",
    "        preds.append(new_char)\n",
    "\n",
    "        if len(preds) > 50:\n",
    "            break\n",
    "        \n",
    "\n",
    "    return \"\".join([out_int_to_token[i] for i in preds[1:-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"../input/Train.csv\")[[\"text\", \"label\"]]\n",
    "train.columns = [\"texts\", \"data_labels\"]\n",
    "\n",
    "data = train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):    #Might use the same setting if they work to other languages (english and french)  \n",
    "\n",
    "    text = text.replace('ß',\"b\")\n",
    "    text = text.replace('à',\"a\")\n",
    "    text = text.replace('á',\"a\")\n",
    "    text = text.replace('ç',\"c\")\n",
    "    text = text.replace('è',\"e\")\n",
    "    text = text.replace('é',\"e\")\n",
    "    text = text.replace('$',\"s\")\n",
    "    text = text.replace(\"1\",\"\")\n",
    "    \n",
    "    \n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^A-Za-z0-9 ,!?.]', '', text)\n",
    "\n",
    "    \n",
    "    # Remove '@name'\n",
    "    text = re.sub(r'(@.*?)[\\s]', ' ', text)\n",
    "\n",
    "    # Replace '&amp;' with '&'\n",
    "    text = re.sub(r'&amp;', '&', text)\n",
    "\n",
    "    # Remove trailing whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    text = re.sub(r'([h][h][h][h])\\1+', r'\\1', text)\n",
    "    text = re.sub(r'([a-g-i-z])\\1+', r'\\1', text)  #Remove repeating characters\n",
    "    text = re.sub(r' [0-9]+ ', \" \", text)\n",
    "    text = re.sub(r'^[0-9]+ ', \"\", text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Keep numbers block\n",
    "def split(text):\n",
    "    \n",
    "    splits = re.findall(r\"[\\w']+|[?!.,]\", text)\n",
    "\n",
    "    to_be_added = []\n",
    "    idx_to_be_added = []\n",
    "    \n",
    "    forbidden = [\"?\", \"!\", \".\", \",\"] + known_idx\n",
    "\n",
    "    for i, split in enumerate(splits):\n",
    "\n",
    "        if split in forbidden:\n",
    "            if split in known_idx:\n",
    "                to_be_added.append(known[split])\n",
    "            else:\n",
    "                to_be_added.append(split)\n",
    "            idx_to_be_added.append(i)\n",
    "        #else:\n",
    "        #splits[i] = splits[i][:1000]\n",
    "\n",
    "\n",
    "    splits = [i for i in splits if not i in forbidden]\n",
    "    \n",
    "    return splits, to_be_added, idx_to_be_added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "problematic = []\n",
    "\n",
    "def convert_phrase_2(text):\n",
    "    text = text.replace(\"0\",\"\")\n",
    "    text = text.replace(\"6\",\"\")\n",
    "\n",
    "    #print(\"\\nTEXT: \"+text)\n",
    "    phrase, to_be_added, idx_to_be_added = split(text.lower())\n",
    "\n",
    "    max_len_phrase = max([len(i) for i in phrase])\n",
    "\n",
    "    input_sentence = []\n",
    "    for word in phrase:\n",
    "        input_sentence.append([in_token_to_int[i] for i in word] + [pad_token]*(max_len_phrase-len(word)))\n",
    "\n",
    "    input_sentence = torch.Tensor(input_sentence).long().T.to(device)\n",
    "    preds = [[sos_token] * len(phrase)]\n",
    "\n",
    "    end_word = len(phrase) * [False]\n",
    "    src_pad_mask = model.make_len_mask_enc(input_sentence)\n",
    "\n",
    "\n",
    "    while not all(end_word):\n",
    "        output_sentence = torch.Tensor(preds).long().to(device)\n",
    "\n",
    "        src = model.pos_encoder(model.encoder(input_sentence))\n",
    "        trg = model.pos_encoder(model.decoder(output_sentence))\n",
    "\n",
    "        memory = model.transformer_encoder(src, None ,src_pad_mask)\n",
    "        output = model.transformer_decoder(tgt = trg, memory = memory, memory_key_padding_mask = src_pad_mask)\n",
    "        \n",
    "        \n",
    "        output = model.fc_out(output)\n",
    "\n",
    "\n",
    "        output = output.argmax(-1)[-1].cpu().detach().numpy()\n",
    "        preds.append(output.tolist())\n",
    "\n",
    "\n",
    "        end_word = (output == eos_token) | end_word\n",
    "        \n",
    "        if len(preds) > 50:\n",
    "            global problematic \n",
    "            \n",
    "            problematic.append(text)\n",
    "            #print(text)\n",
    "            break\n",
    "            \n",
    "    \n",
    "    preds = np.array(preds).T\n",
    "    result = []\n",
    "\n",
    "    for word in preds:\n",
    "\n",
    "        tmp = []\n",
    "        for i in word[1:]:   \n",
    "            if out_int_to_token[i] == \"<eos>\":\n",
    "                break\n",
    "            tmp.append(out_int_to_token[i])\n",
    "\n",
    "        result.append(\"\".join(tmp))\n",
    "\n",
    "        \n",
    "    #Re-add removed punctuation\n",
    "    for item, idx in zip(to_be_added, idx_to_be_added):\n",
    "\n",
    "        if item == \"?\":\n",
    "            item = \"؟\"\n",
    "        elif item == \",\":\n",
    "            item = \"،\"\n",
    "\n",
    "        result.insert(idx, item)\n",
    "        \n",
    "        \n",
    "    result = \" \".join(result)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.texts = train.texts.apply(preprocess)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 700/700 [18:45<00:00,  1.61s/it]\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "step_size = 100\n",
    "\n",
    "texts = train.texts.values.tolist()\n",
    "\n",
    "for i in tqdm(range(0, len(texts), step_size)): \n",
    "    \n",
    "    out = convert_phrase_2(\" lkrb3 \".join(texts[i:i+step_size]))\n",
    "    splitted_sentences = [ex.lstrip().rstrip() for ex in out.split(\" \" + convert_phrase_2(\"lkrb3\") + \" \")]\n",
    "    \n",
    "    if len(splitted_sentences) != len(texts[i:i+step_size]):\n",
    "        print(\"DANGER\")\n",
    "        break\n",
    "    \n",
    "    results.extend(splitted_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"converted\"] = results.copy()\n",
    "train.to_csv(\"train_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"../input/Test.csv\")\n",
    "test.text = test.text.apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 600/600 [05:43<00:00,  1.75it/s]\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "step_size = 50\n",
    "\n",
    "texts = test.text.values.tolist()\n",
    "\n",
    "for i in tqdm(range(0, len(texts), step_size)): \n",
    "    \n",
    "    out = convert_phrase_2(\" lkrb3 \".join(texts[i:i+step_size]))\n",
    "    splitted_sentences = [ex.lstrip().rstrip() for ex in out.split(\" \" + convert_phrase_2(\"lkrb3\") + \" \")]\n",
    "    \n",
    "    if len(splitted_sentences) != len(texts[i:i+step_size]):\n",
    "        print(\"DANGER\")\n",
    "        break\n",
    "    \n",
    "    results.extend(splitted_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[\"converted\"] = results\n",
    "test.to_csv(\"test_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_for_bert(data, tokenizer, preprocess_text, max_len=256):\n",
    "\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    tmp = tokenizer.encode(\"ab\")[-1]\n",
    "\n",
    "    for sentence in data:\n",
    "\n",
    "        encoding = tokenizer.encode(preprocess_text(sentence))\n",
    "\n",
    "        if len(encoding) > max_len:\n",
    "            encoding = encoding[:max_len-1] + [tmp]\n",
    "\n",
    "        in_ids = encoding\n",
    "        att_mask = [1]*len(encoding)\n",
    "        \n",
    "        input_ids.append(in_ids)\n",
    "        attention_masks.append(att_mask)\n",
    "\n",
    "    return input_ids, attention_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertDataset(Dataset):\n",
    "\n",
    "    def __init__(self, data, masks, label=None):\n",
    "        \n",
    "        self.data = data\n",
    "        self.masks = masks\n",
    "        \n",
    "        if label != None:\n",
    "            self.labels = label\n",
    "        else:\n",
    "            self.labels = None\n",
    "        \n",
    "        self.lengths = [len(i) for i in data]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if self.labels !=  None:\n",
    "            return (self.data[idx], self.masks[idx], self.labels[idx], self.lengths[idx])\n",
    "        else:  #For validation\n",
    "            return (self.data[idx], self.masks[idx], None, self.lengths[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_collator(data):\n",
    "    \n",
    "    sentence, mask, label, length = zip(*data)\n",
    "    \n",
    "    tensor_dim = max(length)\n",
    "    \n",
    "    out_sentence = torch.full((len(sentence), tensor_dim), dtype=torch.long, fill_value=pad)\n",
    "    out_mask = torch.zeros(len(sentence), tensor_dim, dtype=torch.long)\n",
    "\n",
    "    for i in range(len(sentence)):\n",
    "        \n",
    "        out_sentence[i][:len(sentence[i])] = torch.Tensor(sentence[i])\n",
    "        out_mask[i][:len(mask[i])] = torch.Tensor(mask[i])\n",
    "    \n",
    "    if label[0] != None:\n",
    "        return (out_sentence, out_mask, torch.Tensor(label).long())\n",
    "    else:\n",
    "        return (out_sentence, out_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KSampler(Sampler):\n",
    "\n",
    "    def __init__(self, data_source, batch_size):\n",
    "        self.lens = [x[1] for x in data_source]\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __iter__(self):\n",
    "\n",
    "        idx = list(range(len(self.lens)))\n",
    "        arr = list(zip(self.lens, idx))\n",
    "\n",
    "        random.shuffle(arr)\n",
    "        n = self.batch_size*100\n",
    "\n",
    "        iterator = []\n",
    "\n",
    "        for i in range(0, len(self.lens), n):\n",
    "            dt = arr[i:i+n]\n",
    "            dt = sorted(dt, key=lambda x: x[0])\n",
    "\n",
    "            for j in range(0, len(dt), self.batch_size):\n",
    "                indices = list(map(lambda x: x[1], dt[j:j+self.batch_size]))\n",
    "                iterator.append(indices)\n",
    "\n",
    "        random.shuffle(iterator)\n",
    "        return iter([item for sublist in iterator for item in sublist])  #Flatten nested list\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.lens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the BertClassfier class\n",
    "class BertClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, model_name, dropout, freeze_bert=False):\n",
    "\n",
    "        super(BertClassifier, self).__init__()\n",
    "        D_in, H, D_out = 768, 200, 3\n",
    "\n",
    "        self.bert = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(D_in, H),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(H, D_out)\n",
    "        )\n",
    "\n",
    "        if freeze_bert:\n",
    "            for param in self.bert.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "\n",
    "        outputs = self.bert(input_ids=input_ids,\n",
    "                            attention_mask=attention_mask)\n",
    "        \n",
    "        last_hidden_state_cls = outputs[0][:, 0, :]\n",
    "\n",
    "        logits = self.classifier(last_hidden_state_cls)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model(model_name, epochs=4, dropout=0.1):\n",
    "\n",
    "    bert_classifier = BertClassifier(model_name, dropout=dropout, freeze_bert=False)\n",
    "\n",
    "    bert_classifier.to(device)\n",
    "\n",
    "    optimizer = AdamW(bert_classifier.parameters(),\n",
    "                      lr=5e-5,   \n",
    "                      eps=1e-8 \n",
    "                      )\n",
    "\n",
    "    total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                                num_warmup_steps=0, # Default value\n",
    "                                                num_training_steps=total_steps)\n",
    "    return bert_classifier, optimizer, scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "def train(model, train_dataloader, val_dataloader=None, epochs=4, evaluation=False, fold=0, prefix=\"\"):\n",
    "    \n",
    "    global max_acc\n",
    "\n",
    "    print(\"Start training...\\n\")\n",
    "    for epoch_i in range(epochs):\n",
    "\n",
    "        print(f\"{'Epoch':^7} | {'Batch':^7} | {'Train Loss':^12} | {'Val Loss':^10} | {'Val Acc':^9} | {'Elapsed':^9}\")\n",
    "        print(\"-\"*70)\n",
    "\n",
    "        t0_epoch, t0_batch = time.time(), time.time()\n",
    "\n",
    "        total_loss, batch_loss, batch_counts = 0, 0, 0\n",
    "        model.train()\n",
    "\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            batch_counts +=1\n",
    "\n",
    "            b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n",
    "\n",
    "            model.zero_grad()\n",
    "\n",
    "            logits = model(b_input_ids, b_attn_mask)\n",
    "\n",
    "            loss = loss_fn(logits, b_labels)\n",
    "            batch_loss += loss.item()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            if (step % 20 == 0 and step != 0) or (step == len(train_dataloader) - 1):\n",
    "\n",
    "                time_elapsed = time.time() - t0_batch\n",
    "\n",
    "                print(f\"{epoch_i + 1:^7} | {step:^7} | {batch_loss / batch_counts:^12.6f} | {'-':^10} | {'-':^9} | {time_elapsed:^9.2f}\")\n",
    "\n",
    "                batch_loss, batch_counts = 0, 0\n",
    "                t0_batch = time.time()\n",
    "                \n",
    "            if step%200 == 0 and step != 0 and epoch_i != 0 and epoch_i != 1:\n",
    "                \n",
    "                print(\"-\"*70)\n",
    "\n",
    "                if evaluation == True:\n",
    "\n",
    "                    val_loss, val_accuracy = evaluate(model, val_dataloader)\n",
    "                    \n",
    "                    if val_accuracy > max_acc:\n",
    "                        max_acc = val_accuracy\n",
    "                        torch.save(model, prefix + \"_best_\"+str(fold))\n",
    "                        print(\"new max\")\n",
    "                        \n",
    "\n",
    "                    print(val_accuracy)\n",
    "                    \n",
    "                    print(\"-\"*70)\n",
    "                print(\"\\n\")\n",
    "                \n",
    "                model.train()\n",
    "\n",
    "        avg_train_loss = total_loss / len(train_dataloader)\n",
    "\n",
    "        print(\"-\"*70)\n",
    "\n",
    "        if evaluation == True:\n",
    "            \n",
    "            val_loss, val_accuracy = evaluate(model, val_dataloader)\n",
    "            \n",
    "            if val_accuracy > max_acc:\n",
    "                max_acc = val_accuracy\n",
    "                torch.save(model, prefix+\"_best_\"+str(fold))\n",
    "                print(\"new max\")\n",
    "\n",
    "            time_elapsed = time.time() - t0_epoch\n",
    "            \n",
    "            print(f\"{epoch_i + 1:^7} | {'-':^7} | {avg_train_loss:^12.6f} | {val_loss:^10.6f} | {val_accuracy:^9.2f} | {time_elapsed:^9.2f}\")\n",
    "            print(\"-\"*70)\n",
    "        print(\"\\n\")\n",
    "    \n",
    "    print(\"Training complete!\")\n",
    "\n",
    "\n",
    "def evaluate(model, val_dataloader):\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    val_accuracy = []\n",
    "    val_loss = []\n",
    "\n",
    "    for batch in val_dataloader:\n",
    "\n",
    "        b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = model(b_input_ids, b_attn_mask)\n",
    "\n",
    "        loss = loss_fn(logits, b_labels)\n",
    "        val_loss.append(loss.item())\n",
    "\n",
    "        preds = torch.argmax(logits, dim=1).flatten()\n",
    "\n",
    "        accuracy = (preds == b_labels).cpu().numpy().mean() * 100\n",
    "        val_accuracy.append(accuracy)\n",
    "\n",
    "    val_loss = np.mean(val_loss)\n",
    "    val_accuracy = np.mean(val_accuracy)\n",
    "\n",
    "    return val_loss, val_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_indices(arr, idxs):  #Helper function to get multiple indexes from a list\n",
    "    \n",
    "    output = []\n",
    "    for idx in idxs:\n",
    "        output.append(arr[idx])\n",
    "        \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tried these different preprocessing functions and tesed their effect on the results\n",
    "#Found out that text_preprocessing_2 gives the best results for the English model\n",
    "def text_preprocessing_1(text): \n",
    "\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "            \n",
    "    return text\n",
    "\n",
    "\n",
    "def text_preprocessing_2(text): \n",
    "\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    text = re.sub(r'([a-g-i-z][a-g-i-z])\\1+', r'\\1', text)\n",
    "        \n",
    "    return text\n",
    "\n",
    "\n",
    "def text_preprocessing_3(text):    \n",
    "\n",
    "    text = text.replace('ß',\"b\")\n",
    "    text = text.replace('à',\"a\")\n",
    "    text = text.replace('á',\"a\")\n",
    "    text = text.replace('ç',\"c\")\n",
    "    text = text.replace('è',\"e\")\n",
    "    text = text.replace('é',\"e\")\n",
    "    text = text.replace('$',\"s\")\n",
    "    text = text.replace(\"1\",\"\")\n",
    "    \n",
    "    \n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^A-Za-z0-9 ,!?.]', '', text)\n",
    "\n",
    "    \n",
    "    # Remove '@name'\n",
    "    text = re.sub(r'(@.*?)[\\s]', ' ', text)\n",
    "\n",
    "    # Replace '&amp;' with '&'\n",
    "    text = re.sub(r'&amp;', '&', text)\n",
    "\n",
    "    # Remove trailing whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    text = re.sub(r'([h][h][h][h])\\1+', r'\\1', text)\n",
    "    text = re.sub(r'([a-g-i-z])\\1+', r'\\1', text)  #Remove repeating characters\n",
    "    text = re.sub(r' [0-9]+ ', \" \", text)\n",
    "    text = re.sub(r'^[0-9]+ ', \"\", text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1022 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"../input/Train.csv\")[[\"text\", \"label\"]].iloc[1000:]\n",
    "data.columns = [\"texts\", \"data_labels\"]\n",
    "\n",
    "data.data_labels = data.data_labels.replace(0,2)  #Neutral 2, Positive 1, Negative 0\n",
    "data.data_labels = data.data_labels.replace(-1,0)\n",
    "\n",
    "\n",
    "\n",
    "X = data.texts.values\n",
    "y = data.data_labels.values\n",
    "\n",
    "preprocessed_data, masks = preprocessing_for_bert(X, tokenizer_en, text_preprocessing_2, max_len=256)\n",
    "pad = tokenizer_en.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\tFOLD 0 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/home/aigeek/anaconda3/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   1    |   20    |   0.872588   |     -      |     -     |   49.84  \n",
      "   1    |   40    |   0.818342   |     -      |     -     |   48.16  \n",
      "   1    |   60    |   0.758008   |     -      |     -     |   47.36  \n",
      "   1    |   80    |   0.664637   |     -      |     -     |  305.08  \n",
      "   1    |   100   |   0.707218   |     -      |     -     |   53.08  \n",
      "   1    |   120   |   0.659626   |     -      |     -     |   58.64  \n",
      "   1    |   140   |   0.653943   |     -      |     -     |   47.96  \n",
      "   1    |   160   |   0.635301   |     -      |     -     |   47.60  \n",
      "   1    |   180   |   0.588561   |     -      |     -     |   61.27  \n",
      "   1    |   200   |   0.530777   |     -      |     -     |   57.35  \n",
      "   1    |   220   |   0.644613   |     -      |     -     |   46.47  \n",
      "   1    |   240   |   0.629116   |     -      |     -     |   50.72  \n",
      "   1    |   260   |   0.570215   |     -      |     -     |   51.82  \n",
      "   1    |   280   |   0.599692   |     -      |     -     |   57.38  \n"
     ]
    }
   ],
   "source": [
    "#kfold = KFold(5, True, seed)\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "fold = 0\n",
    "\n",
    "bests = []\n",
    "\n",
    "for train_ids, val_ids in kfold.split(preprocessed_data):\n",
    "    \n",
    "    print(\"\\n\\tFOLD %d \\n\" % (fold))\n",
    "    max_acc = -99\n",
    "\n",
    "    X_train = get_indices(preprocessed_data, train_ids)\n",
    "    y_train = get_indices(y, train_ids)\n",
    "    train_masks = get_indices(masks, train_ids)\n",
    "    \n",
    "    X_val = get_indices(preprocessed_data, val_ids)\n",
    "    y_val = get_indices(y, val_ids)\n",
    "    val_masks = get_indices(masks, val_ids)\n",
    "    \n",
    "    \n",
    "    X_val, y_val, val_masks = list(zip(*sorted(zip(X_val, y_val, val_masks), key=lambda x: len(x[0]))))  #Order the validation data for faster validation\n",
    "    X_val, y_val, val_masks = list(X_val), list(y_val), list(val_masks)\n",
    "    \n",
    "    \n",
    "    # Convert other data types to torch.Tensor\n",
    "    y_train = torch.tensor(y_train)\n",
    "    y_val = torch.tensor(y_val)\n",
    "\n",
    "    # Create the DataLoader for our training set\n",
    "    train_data = BertDataset(X_train, train_masks, y_train)\n",
    "    train_sampler = KSampler(train_data, batch_size)\n",
    "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size, collate_fn=data_collator)\n",
    "\n",
    "    # Create the DataLoader for our validation set\n",
    "    val_data = BertDataset(X_val, val_masks, y_val)\n",
    "    val_sampler = SequentialSampler(val_data)\n",
    "    val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size, collate_fn=data_collator)\n",
    "    \n",
    "    \n",
    "    set_seed()    # Set seed for reproducibility\n",
    "    bert_classifier, optimizer, scheduler = initialize_model(model_name=model_name_en, epochs=n_epochs, dropout=0.05)\n",
    "    train(bert_classifier, train_dataloader, val_dataloader, epochs=n_epochs, evaluation=True, fold=fold, prefix=\"en\")\n",
    "    \n",
    "    fold += 1\n",
    "    bests.append(max_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"train_data.csv\")[[\"converted\", \"data_labels\"]].iloc[1000:]\n",
    "data.columns = [\"texts\", \"data_labels\"]\n",
    "\n",
    "data.data_labels = data.data_labels.replace(0,2)  #Neutral 2, Positive 1, Negative 0\n",
    "data.data_labels = data.data_labels.replace(-1,0)\n",
    "\n",
    "\n",
    "\n",
    "X = data.texts.values\n",
    "y = data.data_labels.values\n",
    "\n",
    "preprocessed_data, masks = preprocessing_for_bert(X, tokenizer_ar, lambda x: x, max_len=256)\n",
    "pad = tokenizer_ar.pad_token_id"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
